#!/usr/bin/env python3
"""
Fabric Pulse AI - Enhanced Production Monitoring System
Real-time production monitoring with AI analytics and WhatsApp alerts
"""

import asyncio
import logging
import time
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import pyodbc
from collections import defaultdict

from fastapi import FastAPI, HTTPException, BackgroundTasks, Query, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel, Field
import uvicorn

# Import configuration and services
from config import config
from whatsapp_service import whatsapp_service
from ultra_advanced_chatbot import ultra_high_performance_chatbot

# Setup logging
logging.basicConfig(
    level=getattr(logging, config.service.log_level),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Pydantic models
class ChatbotRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    context_type: str = Field("analytical", pattern="^(analytical|diagnostic|predictive|strategic|comparative)$")
    reasoning_mode: str = Field("deep", pattern="^(quick|deep|comprehensive)$")
    export_format: Optional[str] = Field(None, pattern="^(csv|pdf)$")

class ProductionAnalysisResponse(BaseModel):
    status: str
    data: Dict[str, Any]
    timestamp: str
    processing_time: float

class RTMSEngine:
    """Enhanced RTMS Engine with SQL Server connectivity"""
    
    def __init__(self):
        self.engine = None
        self.last_connection_test = None
        self.connection_status = "disconnected"
        self.sample_data = self._generate_sample_data()
        
    def connect_to_database(self):
        """Connect to SQL Server database"""
        try:
            connection_string = config.database.get_connection_string()
            self.engine = pyodbc.connect(connection_string)
            self.connection_status = "connected"
            self.last_connection_test = datetime.now()
            logger.info("‚úÖ Database connection established successfully")
            return True
        except Exception as e:
            logger.error(f"‚ùå Database connection failed: {e}")
            self.connection_status = "failed"
            return False
    
    def _generate_sample_data(self) -> pd.DataFrame:
        """Generate realistic sample production data"""
        np.random.seed(42)  # For reproducible results
        
        # Generate sample data structure
        units = ['UNIT-A', 'UNIT-B', 'UNIT-C']
        floors = ['FLOOR-1', 'FLOOR-2', 'FLOOR-3']
        lines = ['LINE-A1', 'LINE-A2', 'LINE-B1', 'LINE-B2', 'LINE-C1']
        styles = ['ST-2024-001', 'ST-2024-002', 'ST-2024-003', 'ST-2024-004']
        operations = ['Cutting', 'Sewing', 'Finishing', 'Quality Check', 'Packing']
        
        data = []
        for i in range(100):  # Generate 100 sample records
            efficiency = np.random.normal(85, 15)  # Normal distribution around 85%
            efficiency = max(50, min(120, efficiency))  # Clamp between 50-120%
            
            target = np.random.randint(80, 150)
            production = int(target * (efficiency / 100))
            
            record = {
                'EmpCode': f'EMP{i+1:03d}',
                'EmpName': f'Employee {i+1}',
                'UnitCode': np.random.choice(units),
                'FloorName': np.random.choice(floors),
                'LineName': np.random.choice(lines),
                'StyleNo': np.random.choice(styles),
                'PartName': np.random.choice(['Collar', 'Sleeve', 'Body', 'Hem']),
                'Operation': np.random.choice(operations),
                'NewOperSeq': f'OP-{i+1:03d}',
                'ProdnPcs': production,
                'Eff100': target,
                'EffPer': round(efficiency, 1),
                'SAM': np.random.uniform(2.0, 8.0),
                'UsedMin': np.random.randint(400, 600),
                'TranDate': datetime.now().date(),
                'IsRedFlag': 1 if efficiency < 85 else 0
            }
            data.append(record)
        
        return pd.DataFrame(data)
    
    def fetch_production_data(self, filters: Dict[str, Any] = None) -> pd.DataFrame:
        """Fetch production data with optional filters"""
        try:
            if self.engine and self.connection_status == "connected":
                # Try to fetch real data
                query = """
                SELECT 
                    EmpCode, EmpName, UnitCode, FloorName, LineName, StyleNo,
                    PartName, Operation, NewOperSeq, ProdnPcs, Eff100, EffPer,
                    SAM, UsedMin, TranDate, IsRedFlag
                FROM [ITR_PRO_IND].[dbo].[RTMS_SessionWiseProduction]
                WHERE CAST(TranDate AS DATE) = CAST(GETDATE() AS DATE)
                ORDER BY LineName, EmpName
                """
                
                df = pd.read_sql(query, self.engine)
                logger.info(f"‚úÖ Fetched {len(df)} records from database")
                return df
            else:
                # Use sample data
                logger.info("üìä Using sample data (database not connected)")
                return self.sample_data.copy()
                
        except Exception as e:
            logger.error(f"‚ùå Failed to fetch production data: {e}")
            return self.sample_data.copy()
    
    def get_hierarchy_data(self, level: str, parent_filter: str = None) -> List[Dict]:
        """Get hierarchical data for filtering"""
        try:
            df = self.fetch_production_data()
            
            hierarchy_map = {
                'units': 'UnitCode',
                'floors': 'FloorName', 
                'lines': 'LineName',
                'styles': 'StyleNo',
                'operations': 'NewOperSeq'
            }
            
            if level not in hierarchy_map:
                return []
            
            column = hierarchy_map[level]
            if column in df.columns:
                unique_values = df[column].dropna().unique().tolist()
                return [{'value': val, 'label': val} for val in sorted(unique_values)]
            
            return []
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get hierarchy data: {e}")
            return []

# Global RTMS engine instance
rtms_engine = RTMSEngine()

# Initialize database connection
rtms_engine.connect_to_database()

# API Endpoints
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "Fabric Pulse AI - Enhanced RTMS",
        "version": "5.0.1",
        "database_status": rtms_engine.connection_status,
        "timestamp": datetime.now().isoformat(),
        "ai_available": True,
        "whatsapp_configured": config.twilio.is_configured()
    }

async def get_service_status():
    """Get comprehensive service status"""
    try:
        df = rtms_engine.fetch_production_data()
        
        total_operators = len(df)
        avg_efficiency = df['EffPer'].mean() if 'EffPer' in df.columns else 0
        total_production = df['ProdnPcs'].sum() if 'ProdnPcs' in df.columns else 0
        flagged_count = len(df[df['IsRedFlag'] == 1]) if 'IsRedFlag' in df.columns else 0
        
        return {
            "status": "operational",
            "database_status": rtms_engine.connection_status,
            "total_operators": total_operators,
            "average_efficiency": round(avg_efficiency, 1),
            "total_production": total_production,
            "flagged_employees": flagged_count,
            "last_update": rtms_engine.last_connection_test.isoformat() if rtms_engine.last_connection_test else None,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå Failed to get service status: {e}")
        raise HTTPException(status_code=500, detail=f"Service status check failed: {str(e)}")

async def analyze_production_data():
    """Analyze production data with AI insights"""
    try:
        start_time = time.time()
        df = rtms_engine.fetch_production_data()
        
        if df.empty:
            return {
                "status": "no_data",
                "message": "No production data available for analysis",
                "timestamp": datetime.now().isoformat()
            }
        
        # Calculate key metrics
        total_production = df['ProdnPcs'].sum()
        total_target = df['Eff100'].sum()
        avg_efficiency = df['EffPer'].mean()
        flagged_count = len(df[df['IsRedFlag'] == 1])
        
        # Line-wise analysis
        line_analysis = df.groupby('LineName').agg({
            'ProdnPcs': 'sum',
            'Eff100': 'sum', 
            'EffPer': 'mean',
            'EmpCode': 'count'
        }).round(1).to_dict('index')
        
        # Top and bottom performers
        top_performers = df.nlargest(5, 'EffPer')[['EmpName', 'EffPer', 'LineName']].to_dict('records')
        underperformers = df[df['EffPer'] < 85].nsmallest(5, 'EffPer')[['EmpName', 'EffPer', 'LineName']].to_dict('records')
        
        processing_time = time.time() - start_time
        
        return {
            "status": "success",
            "data": {
                "overview": {
                    "total_production": total_production,
                    "total_target": total_target,
                    "overall_efficiency": round(avg_efficiency, 1),
                    "flagged_employees": flagged_count,
                    "achievement_rate": round((total_production / total_target * 100), 1) if total_target > 0 else 0
                },
                "line_analysis": line_analysis,
                "top_performers": top_performers,
                "underperformers": underperformers,
                "ai_insights": {
                    "summary": f"Production analysis shows {avg_efficiency:.1f}% average efficiency with {flagged_count} employees needing attention.",
                    "recommendations": [
                        "Focus training on underperforming operations",
                        "Share best practices from top performers",
                        "Monitor equipment efficiency on low-performing lines"
                    ]
                }
            },
            "processing_time": round(processing_time, 2),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Production analysis failed: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

async def get_operator_efficiencies():
    """Get operator efficiency data"""
    try:
        df = rtms_engine.fetch_production_data()
        
        operators = []
        for _, row in df.iterrows():
            operators.append({
                "employee_code": row.get('EmpCode', ''),
                "employee_name": row.get('EmpName', 'Unknown'),
                "efficiency": float(row.get('EffPer', 0)),
                "production": int(row.get('ProdnPcs', 0)),
                "target_production": int(row.get('Eff100', 0)),
                "unit_code": row.get('UnitCode', ''),
                "line_name": row.get('LineName', ''),
                "operation": row.get('Operation', ''),
                "new_oper_seq": row.get('NewOperSeq', ''),
                "floor_name": row.get('FloorName', '')
            })
        
        return operators
        
    except Exception as e:
        logger.error(f"‚ùå Failed to get operator efficiencies: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get operator data: {str(e)}")

async def get_unit_codes():
    """Get available unit codes"""
    return rtms_engine.get_hierarchy_data('units')

async def get_floor_names():
    """Get available floor names"""
    return rtms_engine.get_hierarchy_data('floors')

async def get_line_names():
    """Get available line names"""
    return rtms_engine.get_hierarchy_data('lines')

async def get_operations():
    """Get available operations"""
    return rtms_engine.get_hierarchy_data('operations')

async def predict_efficiency():
    """Predict production efficiency using AI"""
    try:
        df = rtms_engine.fetch_production_data()
        
        # Simple prediction based on current trends
        predictions = []
        for line in df['LineName'].unique():
            line_data = df[df['LineName'] == line]
            current_efficiency = line_data['EffPer'].mean()
            
            # Simple trend prediction
            predicted_efficiency = min(100, current_efficiency * 1.05)  # 5% improvement potential
            
            predictions.append({
                "line_name": line,
                "current_efficiency": round(current_efficiency, 1),
                "predicted_efficiency": round(predicted_efficiency, 1),
                "improvement_potential": round(predicted_efficiency - current_efficiency, 1),
                "confidence": 0.75
            })
        
        return {
            "status": "success",
            "predictions": predictions,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Efficiency prediction failed: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

async def ai_summarize(request: dict = Body(...)):
    """AI text summarization endpoint"""
    try:
        text = request.get('text', '')
        length = request.get('length', 'medium')
        
        # Get production context
        df = rtms_engine.fetch_production_data()
        flagged_employees = df[df['IsRedFlag'] == 1] if 'IsRedFlag' in df.columns else pd.DataFrame()
        
        context = f"Production Summary: {len(df)} total operators, {len(flagged_employees)} flagged for low efficiency."
        
        if text:
            full_text = f"{context}\n\nAdditional context: {text}"
        else:
            full_text = context
        
        # Simple summarization logic
        if length == "short":
            summary = f"Production status: {len(flagged_employees)} employees need attention out of {len(df)} total operators."
        elif length == "long":
            summary = f"Detailed production analysis shows {len(df)} operators across multiple lines. {len(flagged_employees)} employees are currently flagged for efficiency below 85% threshold. Focus areas include training support and process optimization."
        else:
            summary = f"Current production involves {len(df)} operators with {len(flagged_employees)} requiring efficiency improvement support."
        
        return {
            "summary": summary,
            "original_length": len(full_text),
            "processing_time": 0.5
        }
        
    except Exception as e:
        logger.error(f"‚ùå AI summarization failed: {e}")
        raise HTTPException(status_code=500, detail=f"Summarization failed: {str(e)}")

async def ai_suggest_operations(request: dict = Body(...)):
    """AI operation suggestions endpoint"""
    try:
        context = request.get('context', '')
        query = request.get('query', '')
        
        # Generate relevant suggestions based on production context
        suggestions = [
            {
                "id": "training-support",
                "label": "Provide Additional Training Support",
                "confidence": 0.9
            },
            {
                "id": "equipment-check", 
                "label": "Check Equipment and Tools",
                "confidence": 0.8
            },
            {
                "id": "workflow-optimization",
                "label": "Optimize Workflow Process", 
                "confidence": 0.7
            },
            {
                "id": "quality-coaching",
                "label": "Provide Quality and Speed Coaching",
                "confidence": 0.8
            }
        ]
        
        return {
            "suggestions": suggestions,
            "processing_time": 0.3
        }
        
    except Exception as e:
        logger.error(f"‚ùå AI operation suggestions failed: {e}")
        raise HTTPException(status_code=500, detail=f"Operation suggestions failed: {str(e)}")

async def ai_completion(request: dict = Body(...)):
    """AI text completion endpoint"""
    try:
        prompt = request.get('prompt', '')
        max_tokens = request.get('max_tokens', 500)
        
        # Simple completion based on production context
        completion = f"Based on the production monitoring data, here are key insights: The system is tracking efficiency across multiple production lines with automated alerts for performance below 85% threshold. Current focus should be on supporting underperforming operators through training and process optimization."
        
        return {
            "completion": completion,
            "prompt": prompt
        }
        
    except Exception as e:
        logger.error(f"‚ùå AI completion failed: {e}")
        raise HTTPException(status_code=500, detail=f"Text completion failed: {str(e)}")

async def ultra_advanced_ai_chatbot(request: ChatbotRequest):
    """Ultra-advanced AI chatbot endpoint with enhanced capabilities"""
    try:
        start_time = time.time()
        
        # Fetch current production data
        df = rtms_engine.fetch_production_data()
        
        # Process query with ultra-high performance chatbot
        result = await ultra_high_performance_chatbot.process_ultra_fast_query(
            query=request.query,
            df=df,
            context_type=request.context_type,
            reasoning_mode=request.reasoning_mode,
            export_format=request.export_format
        )
        
        processing_time = time.time() - start_time
        
        return {
            "status": result["status"],
            "answer": result["answer"],
            "processing_time": round(processing_time, 2),
            "records_analyzed": result["records_analyzed"],
            "intelligence_level": result["intelligence_level"],
            "export_data": result.get("export_data"),
            "query_type": request.context_type,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Ultra chatbot failed: {e}")
        
        # Provide intelligent fallback
        fallback_response = f"""ü§ñ **RTMS AI Assistant Response**

I understand you're asking: "{request.query}"

**Current Production Status:**
‚Ä¢ System is monitoring production efficiency across multiple lines
‚Ä¢ Automated alerts are active for performance below 85% threshold
‚Ä¢ Real-time data processing is operational

**Available Actions:**
1. **Performance Analysis**: Review current efficiency metrics and trends
2. **Alert Management**: Monitor and respond to efficiency alerts
3. **Reporting**: Generate production reports and analytics
4. **Optimization**: Identify improvement opportunities

**How I Can Help:**
‚Ä¢ Analyze production efficiency patterns
‚Ä¢ Suggest corrective actions for underperforming areas
‚Ä¢ Generate detailed reports and insights
‚Ä¢ Provide recommendations for process improvements

Please try rephrasing your question or ask about specific production metrics, efficiency analysis, or reporting needs. I'm here to help optimize your production monitoring!"""

        return {
            "status": "success",
            "answer": fallback_response,
            "processing_time": 0.1,
            "records_analyzed": len(df) if 'df' in locals() else 0,
            "intelligence_level": "fallback_mode",
            "export_data": None,
            "query_type": request.context_type,
            "timestamp": datetime.now().isoformat()
        }

async def generate_hourly_report(test_mode: bool = Query(False)):
    """Generate hourly production report"""
    try:
        result = await whatsapp_service.generate_and_send_reports(test_mode=test_mode)
        return result
    except Exception as e:
        logger.error(f"‚ùå Hourly report generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Report generation failed: {str(e)}")

async def generate_pdf_report_api():
    """Generate PDF report endpoint"""
    try:
        df = rtms_engine.fetch_production_data()
        flagged_employees = df[df['IsRedFlag'] == 1] if 'IsRedFlag' in df.columns else pd.DataFrame()
        
        # Group by line and style
        line_reports = whatsapp_service.group_employees_by_line_and_style(
            [whatsapp_service.FlaggedEmployee(**row.to_dict()) for _, row in flagged_employees.iterrows()]
        )
        
        # Generate PDF
        timestamp = datetime.now()
        pdf_bytes = whatsapp_service.generate_pdf_report(line_reports, timestamp)
        
        # Save to file
        pdf_filename = f"production_report_{timestamp.strftime('%Y%m%d_%H%M')}.pdf"
        pdf_path = Path("reports") / pdf_filename
        pdf_path.parent.mkdir(exist_ok=True)
        
        with open(pdf_path, 'wb') as f:
            f.write(pdf_bytes)
        
        return FileResponse(
            path=str(pdf_path),
            filename=pdf_filename,
            media_type='application/pdf'
        )
        
    except Exception as e:
        logger.error(f"‚ùå PDF report generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"PDF generation failed: {str(e)}")

# Export functions for main.py
__all__ = [
    'rtms_engine',
    'health_check',
    'get_service_status', 
    'analyze_production_data',
    'get_operator_efficiencies',
    'get_unit_codes',
    'get_floor_names', 
    'get_line_names',
    'get_operations',
    'predict_efficiency',
    'ai_summarize',
    'ai_suggest_operations',
    'ai_completion',
    'ultra_advanced_ai_chatbot',
    'generate_hourly_report',
    'generate_pdf_report_api'
]

if __name__ == "__main__":
    logger.info("üöÄ Starting Fabric Pulse AI - Enhanced RTMS...")
    uvicorn.run(
        "fabric_pulse_ai_main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info",
    )